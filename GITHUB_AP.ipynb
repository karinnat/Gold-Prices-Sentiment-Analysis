{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from tensorflow.keras import preprocessing as kprocessing\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "import transformers\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import LSTM\n",
    "from keras import layers \n",
    "\n",
    "# Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import contractions\n",
    "import string \n",
    "\n",
    "# Various\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('XAU/USD since:2020-03-29 until:2022-08-24').get_items()):\n",
    "    if i > 500000000:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "    print (i)\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "df_tweets = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tweets = pd.read_csv('Twitter_past_6months', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[df_tweets['Datetime'] < '2022-08-23 08:30']\n",
    "df_tweets = df_tweets[df_tweets['Datetime'] > '2022-03-30 21:30']\n",
    "df_tweets.reindex()\n",
    "df_tweets = df_tweets.reset_index(drop = True)\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30 = pd.read_excel('/Users/karinnathea/Downloads/Intraday_Price_Data.xlsx', '30 min')\n",
    "df_30['Return'] = df_30['Close'].pct_change()\n",
    "df_30 = df_30.dropna()\n",
    "df_30 = calculate_binary(df_30)\n",
    "df_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf933b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_binary(data):\n",
    "    data['Binary'] = \"\"\n",
    "    for i in range(len(data)):\n",
    "        if (data['Return'].iloc[i] > 0):\n",
    "            data['Binary'].iloc[i] = 1\n",
    "        else:\n",
    "            data['Binary'].iloc[i] = 0\n",
    "    return (data)\n",
    "\n",
    "df_30 = calculate_binary(df_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45441c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing data\n",
    "\n",
    "df_tweets['Converted_Text'] = df_tweets['Text'].apply(lambda x: re.sub('(http[s]?):\\/\\/\\S+', '', x))\n",
    " \n",
    "# remove hashtags\n",
    "df_tweets['Converted_Text'] = df_tweets['Converted_Text'].apply(lambda x: re.sub('#+', ' ', x))\n",
    " \n",
    "# convert to lower case\n",
    "df_tweets['Converted_Text'] = df_tweets['Converted_Text'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove punctuations\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "df_tweets['Converted_Text'] = df_tweets['Converted_Text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# apply contractions\n",
    "df_tweets['Converted_Text'] = df_tweets['Converted_Text'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "\n",
    "## Tokenisation\n",
    "def tokenisation(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "df_tweets['Token'] = df_tweets['Converted_Text'].apply(lambda x: tokenisation(x.lower()))\n",
    "\n",
    "def stemming(text):\n",
    "    text = [nltk.PorterStemmer().stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df_tweets['Token'] = df_tweets['Token'].apply(lambda x: stemming(x))\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "df_tweets['Token'] = df_tweets['Token'].apply(lemmatizer_on_text)\n",
    "\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fde767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the time of year, month, day and minute to each dataframe\n",
    "def extract_year(datetime):\n",
    "    return datetime.date().year\n",
    "\n",
    "def extract_month(datetime):\n",
    "    return datetime.date().month\n",
    "\n",
    "def extract_day(datetime):\n",
    "    return datetime.date().day\n",
    "\n",
    "def extract_hour(datetime):\n",
    "    return datetime.hour\n",
    "\n",
    "def extract_minute(datetime):\n",
    "    return datetime.minute\n",
    "\n",
    "\n",
    "df_tweets['year'] = df_tweets['Datetime'].apply(extract_year)\n",
    "df_tweets['month'] = df_tweets['Datetime'].apply(extract_month)\n",
    "df_tweets['day'] = df_tweets['Datetime'].apply(extract_day)\n",
    "df_tweets['hour'] = df_tweets['Datetime'].apply(extract_hour)\n",
    "df_tweets['min'] = df_tweets['Datetime'].apply(extract_minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30['year'] = df_30['Date'].apply(extract_year)\n",
    "df_30['month'] = df_30['Date'].apply(extract_month)\n",
    "df_30['day'] = df_30['Date'].apply(extract_day)\n",
    "df_30['hour'] = df_30['Date'].apply(extract_hour)\n",
    "df_30['minute'] = df_30['Date'].apply(extract_minute)\n",
    "df_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2295df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_30min (minute):\n",
    "    if minute in list(range(0,31)):\n",
    "        return 30\n",
    "    if minute in list(range(30,61)):\n",
    "        return 0\n",
    "    \n",
    "df_tweets['minute'] = df_tweets['min'].apply(nearest_30min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8130ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30_merge = df_30[[\"Return\", \"Binary\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n",
    "new_merge = pd.merge(df_tweets, df_30_merge, on=['year', 'month', 'day', 'hour', 'minute'], how='outer')\n",
    "new_df = new_merge_30[:-1]\n",
    "new_df.fillna(method = 'ffill', inplace = True)\n",
    "new_df.to_csv('cleanedData', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ad290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive return \n",
    "data_pos = new_df['Text'][new_df ['Binary'] == 1]\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(data_pos))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# Negative return \n",
    "data_neg = new_df['Text'][new_df ['Binary'] == 1]\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(data_neg))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02ef5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_df = pd.read_csv('cleanedData', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18795fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modelling processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac39776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data from 24th of July 2022 onwards\n",
    "test = new_df[:48906]\n",
    "y_test = test['Binary']\n",
    "x_test = test['Token']\n",
    "# train data from 30th of March 2022 to 22nd of July 2022\n",
    "train = new_df[48906:]\n",
    "y_train = train['Binary']\n",
    "x_train = train['Token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + Unigram + Bigram Vectorisation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features = 5000000)\n",
    "vectoriser.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectoriser.transform(x_train)\n",
    "x_test  = vectoriser.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(x_test)\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    categories = ['0','1']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "    labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "    xticklabels = categories, yticklabels = categories)\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\" , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCmodel = LinearSVC()\n",
    "SVCmodel.fit(x_train, y_train)\n",
    "model_Evaluate(SVCmodel)\n",
    "y_pred1 = SVCmodel.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbe36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFModel = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "RFModel.fit(x_train, y_train)\n",
    "model_Evaluate(RFModel)\n",
    "y_pred2 = LRmodel.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96a32ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM \n",
    "X_tf = new_df['Converted_Text']\n",
    "y_tf_class = new_df['Binary'].astype(str)\n",
    "\n",
    "# Split into training and test data\n",
    "X_tf_train = X_tf[48906:]\n",
    "y_tf_train = y_tf_class[48906:]\n",
    "X_tf_test = X_tf[:48906]\n",
    "y_tf_test = y_tf_class[:48906]\n",
    "\n",
    "y_tf_train = np.asarray(y_tf_train).astype(\"float64\")\n",
    "y_tf_test = np.asarray(y_tf_test).astype(\"float64\")\n",
    "\n",
    "corpus = X_tf_train\n",
    "max_words = 125000\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', num_words=max_words, oov_token=\"<pad>\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "voc = tokenizer.word_index\n",
    "reverse_voc = dict([(value, key) for (key, value) in voc.items()])\n",
    "\n",
    "max_len = 200\n",
    "sequences = tokenizer.texts_to_sequences(X_tf_train)\n",
    "# Convert both vectorised train and test data to sequence \n",
    "X_tf_train_seq = kprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "X_tf_test_seq = kprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(X_tf_test), maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37c91019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d17600e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 100.0% 1662.7/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# This may take several minutes\n",
    "w2v = api.load(\"word2vec-google-news-300\")\n",
    "emb_matrix = np.zeros((max_words+1, 300))\n",
    "for i in range(max_words):\n",
    "    w = reverse_voc[i+1]\n",
    "    if w in w2v:\n",
    "        emb_matrix[i+1,:] = w2v[w]\n",
    "emb_size = emb_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffc526af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "max_words = 125000\n",
    "\n",
    "emb_matrix = np.zeros((max_words + 1, 300))\n",
    "for i in range(max_words):\n",
    "    w = reverse_voc[i+1]\n",
    "    if w in w2v:\n",
    "        emb_matrix[i+1,:] = w2v[w]\n",
    "emb_size = emb_matrix.shape[1]\n",
    "\n",
    "lstm_out = 125\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words+1,emb_size,weights=[emb_matrix],trainable=False, name='embedding'))\n",
    "model.add(layers.Bidirectional(layers.LSTM(lstm_out)))\n",
    "model.add(layers.Dropout(0.2, name='dropout'))\n",
    "model.add(Dense(32, activation='relu', name='dense'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#fit model\n",
    "batch_size = 32\n",
    "model.fit(X_tf_train_seq, y_tf_train, epochs = 10, verbose=1, batch_size=batch_size)\n",
    "\n",
    "#analyze the results\n",
    "score, acc = model.evaluate(X_tf_test_seq, y_tf_test, verbose = 2, batch_size=batch_size)\n",
    "y_pred3 = model.predict(X_tf_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87010c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve,  roc_auc_score, classification_report\n",
    "#ROC AUC curve\n",
    "rocAuc = roc_auc_score(y_tf_test, y_pred3)\n",
    "\n",
    "falsePositiveRate, truePositiveRate, _ = roc_curve(y_tf_test, y_pred3)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(falsePositiveRate, truePositiveRate, color='green',\n",
    "         lw=3, label='ROC curve (area = %0.2f)' % rocAuc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic of Sentiiment Analysis Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Other accuracy metrices\n",
    "y_pred3 = (y_pred3 > 0.5)\n",
    "\n",
    "#confusion metrix\n",
    "cm = confusion_matrix(y_tf_test, y_pred3)\n",
    "print(cm)\n",
    "\n",
    "#F1 Score, Recall and Precision\n",
    "print(classification_report(y_tf_test, y_pred3, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Plotting confusion matrix\n",
    "categories = ['0','1']\n",
    "group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in cm.flatten() / np.sum(cm)]\n",
    "labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot = labels, cmap = 'Blues',fmt = '', xticklabels = categories, yticklabels = categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve,  roc_auc_score, classification_report\n",
    "#ROC AUC curve\n",
    "rocAuc = roc_auc_score(y_tf_test, y_pred4)\n",
    "\n",
    "falsePositiveRate, truePositiveRate, _ = roc_curve(y_tf_test, y_pred4)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(falsePositiveRate, truePositiveRate, color='green',\n",
    "         lw=3, label='ROC curve (area = %0.2f)' % rocAuc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic of Sentiiment Analysis Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Other accuracy metrices\n",
    "y_pred3 = (y_pred3 > 0.5)\n",
    "\n",
    "#confusion metrix\n",
    "cm = confusion_matrix(y_tf_test, y_pred3)\n",
    "print(cm)\n",
    "\n",
    "#F1 Score, Recall and Precision\n",
    "print(classification_report(y_tf_test, y_pred3, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Plotting confusion matrix\n",
    "categories = ['0','1']\n",
    "group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in cm.flatten() / np.sum(cm)]\n",
    "labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot = labels, cmap = 'Blues',fmt = '', xticklabels = categories, yticklabels = categories)\n",
    "plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "plt.ylabel(\"Actual values\" , fontdict = {'size':14}, labelpad = 10)\n",
    "plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
